{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "On the task that we never researched before, we need to have a baseline model to be standard of any model that we will explore in the future.\n",
    "\n",
    "This model is a simple image classification model. The architecture of this model is CNN + FC.\n",
    "\n",
    "We have only 48 hours for this task. We can't completely spend time on any process. We don't have enough time. For this reason, we explored only pretrain-DenseNet121. We trained this model by using transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook, tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from DataPrep import preprocess_image, augment_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train_sets.csv')\n",
    "val_df = pd.read_csv('datasets/val_sets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "      <th>whale_id_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66f164af.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81bd5469.jpg</td>\n",
       "      <td>w_143b201</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193ae7a0.jpg</td>\n",
       "      <td>w_5297ab3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feb15de4.jpg</td>\n",
       "      <td>w_8d46cef</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7fe190ce.jpg</td>\n",
       "      <td>w_43b50e5</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>53c207cf.jpg</td>\n",
       "      <td>w_94cd45e</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>8cc05900.jpg</td>\n",
       "      <td>w_7554f44</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>fa8ed4af.jpg</td>\n",
       "      <td>w_43be268</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>59ecd346.jpg</td>\n",
       "      <td>w_2a04ceb</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>588e85d5.jpg</td>\n",
       "      <td>w_59eb8ae</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2654 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Image         Id  whale_id_index\n",
       "0     66f164af.jpg  new_whale               0\n",
       "1     81bd5469.jpg  w_143b201              20\n",
       "2     193ae7a0.jpg  w_5297ab3              95\n",
       "3     feb15de4.jpg  w_8d46cef             167\n",
       "4     7fe190ce.jpg  w_43b50e5              76\n",
       "...            ...        ...             ...\n",
       "2649  53c207cf.jpg  w_94cd45e             176\n",
       "2650  8cc05900.jpg  w_7554f44             140\n",
       "2651  fa8ed4af.jpg  w_43be268              77\n",
       "2652  59ecd346.jpg  w_2a04ceb              43\n",
       "2653  588e85d5.jpg  w_59eb8ae             103\n",
       "\n",
       "[2654 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, df, dataset_dir, batch_size=32):\n",
    "        self.df = df\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.number_class = self.df['whale_id_index'].max() + 1\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @property\n",
    "    def number_step(self):\n",
    "        return len(self.df) // self.batch_size\n",
    "    \n",
    "    def create_generator(self, shuffle=False, augment=False):\n",
    "        df = self.df\n",
    "        if shuffle:\n",
    "            df = df.sample(frac=1, replace=False)\n",
    "        while True:\n",
    "            for i in range(0, len(df), self.batch_size):\n",
    "                batch_df = df.iloc[i:i+self.batch_size]\n",
    "                \n",
    "                xs = []\n",
    "                ys = []\n",
    "                for _, whale in batch_df.iterrows():\n",
    "                    img = cv2.imread(str(self.dataset_dir / whale['Image']))\n",
    "                    if augment:\n",
    "                        img = augment_pipeline.augment(image=img)\n",
    "                    img = preprocess_image(img)\n",
    "                    \n",
    "                    _class = to_categorical(whale['whale_id_index'], self.number_class)\n",
    "                    \n",
    "                    xs.append(img)\n",
    "                    ys.append(_class)\n",
    "                    \n",
    "                xs = np.array(xs)\n",
    "                ys = np.array(ys)\n",
    "                yield xs, ys\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path('datasets/train/train')\n",
    "\n",
    "train_datagen = DataGenerator(train_df, input_dir)\n",
    "train_gen = train_datagen.create_generator(shuffle=True, augment=False)\n",
    "\n",
    "val_datagen = DataGenerator(val_df, input_dir)\n",
    "val_gen = val_datagen.create_generator(shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building & Trainning\n",
    "\n",
    "freeze CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = DenseNet121(include_top=False,\n",
    "                       weights='imagenet',\n",
    "                       input_shape=(478, 968, 3)\n",
    "                      )\n",
    "model = Sequential([\n",
    "    backbone,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(train_datagen.number_class, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 15, 30, 1024)      7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 313)               320825    \n",
      "=================================================================\n",
      "Total params: 7,358,329\n",
      "Trainable params: 320,825\n",
      "Non-trainable params: 7,037,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(1e-3)\n",
    "model.compile(optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 5.2144 - acc: 0.2269\n",
      "Epoch 00001: val_acc improved from -inf to 0.24688, saving model to benchmark/baseline/freeze-ep-01-val_acc0.2469.hdf5\n",
      "82/82 [==============================] - 55s 668ms/step - loss: 5.2144 - acc: 0.2269 - val_loss: 4.7075 - val_acc: 0.2469\n",
      "Epoch 2/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 4.6283 - acc: 0.2450\n",
      "Epoch 00002: val_acc did not improve from 0.24688\n",
      "82/82 [==============================] - 49s 603ms/step - loss: 4.6283 - acc: 0.2450 - val_loss: 4.5046 - val_acc: 0.2469\n",
      "Epoch 3/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 4.3387 - acc: 0.2452\n",
      "Epoch 00003: val_acc did not improve from 0.24688\n",
      "82/82 [==============================] - 49s 601ms/step - loss: 4.3387 - acc: 0.2452 - val_loss: 4.3367 - val_acc: 0.2469\n",
      "Epoch 4/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 4.0773 - acc: 0.2468\n",
      "Epoch 00004: val_acc did not improve from 0.24688\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 4.0773 - acc: 0.2468 - val_loss: 4.1945 - val_acc: 0.2469\n",
      "Epoch 5/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 3.8553 - acc: 0.2475\n",
      "Epoch 00005: val_acc improved from 0.24688 to 0.25469, saving model to benchmark/baseline/freeze-ep-05-val_acc0.2547.hdf5\n",
      "82/82 [==============================] - 49s 600ms/step - loss: 3.8553 - acc: 0.2475 - val_loss: 4.0707 - val_acc: 0.2547\n",
      "Epoch 6/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 3.6394 - acc: 0.2571\n",
      "Epoch 00006: val_acc improved from 0.25469 to 0.26250, saving model to benchmark/baseline/freeze-ep-06-val_acc0.2625.hdf5\n",
      "82/82 [==============================] - 49s 600ms/step - loss: 3.6394 - acc: 0.2571 - val_loss: 3.9599 - val_acc: 0.2625\n",
      "Epoch 7/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 3.4494 - acc: 0.2635\n",
      "Epoch 00007: val_acc improved from 0.26250 to 0.26562, saving model to benchmark/baseline/freeze-ep-07-val_acc0.2656.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 3.4494 - acc: 0.2635 - val_loss: 3.8584 - val_acc: 0.2656\n",
      "Epoch 8/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 3.2691 - acc: 0.2757\n",
      "Epoch 00008: val_acc improved from 0.26562 to 0.27500, saving model to benchmark/baseline/freeze-ep-08-val_acc0.2750.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 3.2691 - acc: 0.2757 - val_loss: 3.7619 - val_acc: 0.2750\n",
      "Epoch 9/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 3.0933 - acc: 0.2971\n",
      "Epoch 00009: val_acc improved from 0.27500 to 0.28281, saving model to benchmark/baseline/freeze-ep-09-val_acc0.2828.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 3.0933 - acc: 0.2971 - val_loss: 3.6709 - val_acc: 0.2828\n",
      "Epoch 10/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.9409 - acc: 0.3150\n",
      "Epoch 00010: val_acc improved from 0.28281 to 0.28906, saving model to benchmark/baseline/freeze-ep-10-val_acc0.2891.hdf5\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 2.9409 - acc: 0.3150 - val_loss: 3.5857 - val_acc: 0.2891\n",
      "Epoch 11/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.7925 - acc: 0.3341\n",
      "Epoch 00011: val_acc improved from 0.28906 to 0.29375, saving model to benchmark/baseline/freeze-ep-11-val_acc0.2937.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 2.7925 - acc: 0.3341 - val_loss: 3.5074 - val_acc: 0.2937\n",
      "Epoch 12/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.6594 - acc: 0.3600\n",
      "Epoch 00012: val_acc improved from 0.29375 to 0.30000, saving model to benchmark/baseline/freeze-ep-12-val_acc0.3000.hdf5\n",
      "82/82 [==============================] - 49s 595ms/step - loss: 2.6594 - acc: 0.3600 - val_loss: 3.4353 - val_acc: 0.3000\n",
      "Epoch 13/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.5251 - acc: 0.3841\n",
      "Epoch 00013: val_acc did not improve from 0.30000\n",
      "82/82 [==============================] - 49s 592ms/step - loss: 2.5251 - acc: 0.3841 - val_loss: 3.3704 - val_acc: 0.3000\n",
      "Epoch 14/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.4089 - acc: 0.4150\n",
      "Epoch 00014: val_acc did not improve from 0.30000\n",
      "82/82 [==============================] - 48s 591ms/step - loss: 2.4089 - acc: 0.4150 - val_loss: 3.3119 - val_acc: 0.3000\n",
      "Epoch 15/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.2955 - acc: 0.4436\n",
      "Epoch 00015: val_acc did not improve from 0.30000\n",
      "82/82 [==============================] - 49s 592ms/step - loss: 2.2955 - acc: 0.4436 - val_loss: 3.2577 - val_acc: 0.3000\n",
      "Epoch 16/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.1963 - acc: 0.4683\n",
      "Epoch 00016: val_acc improved from 0.30000 to 0.30156, saving model to benchmark/baseline/freeze-ep-16-val_acc0.3016.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 2.1963 - acc: 0.4683 - val_loss: 3.2075 - val_acc: 0.3016\n",
      "Epoch 17/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.1014 - acc: 0.4947\n",
      "Epoch 00017: val_acc improved from 0.30156 to 0.30469, saving model to benchmark/baseline/freeze-ep-17-val_acc0.3047.hdf5\n",
      "82/82 [==============================] - 49s 597ms/step - loss: 2.1014 - acc: 0.4947 - val_loss: 3.1621 - val_acc: 0.3047\n",
      "Epoch 18/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.9986 - acc: 0.5202\n",
      "Epoch 00018: val_acc improved from 0.30469 to 0.31094, saving model to benchmark/baseline/freeze-ep-18-val_acc0.3109.hdf5\n",
      "82/82 [==============================] - 49s 599ms/step - loss: 1.9986 - acc: 0.5202 - val_loss: 3.1205 - val_acc: 0.3109\n",
      "Epoch 19/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.9130 - acc: 0.5442\n",
      "Epoch 00019: val_acc improved from 0.31094 to 0.31250, saving model to benchmark/baseline/freeze-ep-19-val_acc0.3125.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 1.9130 - acc: 0.5442 - val_loss: 3.0800 - val_acc: 0.3125\n",
      "Epoch 20/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.8314 - acc: 0.5709\n",
      "Epoch 00020: val_acc improved from 0.31250 to 0.32188, saving model to benchmark/baseline/freeze-ep-20-val_acc0.3219.hdf5\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 1.8314 - acc: 0.5709 - val_loss: 3.0385 - val_acc: 0.3219\n",
      "Epoch 21/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.7518 - acc: 0.5889\n",
      "Epoch 00021: val_acc improved from 0.32188 to 0.33281, saving model to benchmark/baseline/freeze-ep-21-val_acc0.3328.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 1.7518 - acc: 0.5889 - val_loss: 2.9962 - val_acc: 0.3328\n",
      "Epoch 22/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.6812 - acc: 0.6110\n",
      "Epoch 00022: val_acc improved from 0.33281 to 0.34375, saving model to benchmark/baseline/freeze-ep-22-val_acc0.3438.hdf5\n",
      "82/82 [==============================] - 49s 595ms/step - loss: 1.6812 - acc: 0.6110 - val_loss: 2.9564 - val_acc: 0.3438\n",
      "Epoch 23/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.6071 - acc: 0.6331\n",
      "Epoch 00023: val_acc improved from 0.34375 to 0.35625, saving model to benchmark/baseline/freeze-ep-23-val_acc0.3562.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 1.6071 - acc: 0.6331 - val_loss: 2.9196 - val_acc: 0.3562\n",
      "Epoch 24/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.5488 - acc: 0.6529\n",
      "Epoch 00024: val_acc improved from 0.35625 to 0.37969, saving model to benchmark/baseline/freeze-ep-24-val_acc0.3797.hdf5\n",
      "82/82 [==============================] - 49s 595ms/step - loss: 1.5488 - acc: 0.6529 - val_loss: 2.8852 - val_acc: 0.3797\n",
      "Epoch 25/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.4811 - acc: 0.6732\n",
      "Epoch 00025: val_acc improved from 0.37969 to 0.38906, saving model to benchmark/baseline/freeze-ep-25-val_acc0.3891.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 1.4811 - acc: 0.6732 - val_loss: 2.8531 - val_acc: 0.3891\n",
      "Epoch 26/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.4243 - acc: 0.6903\n",
      "Epoch 00026: val_acc improved from 0.38906 to 0.39375, saving model to benchmark/baseline/freeze-ep-26-val_acc0.3938.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 1.4243 - acc: 0.6903 - val_loss: 2.8240 - val_acc: 0.3938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.3656 - acc: 0.7086\n",
      "Epoch 00027: val_acc improved from 0.39375 to 0.40625, saving model to benchmark/baseline/freeze-ep-27-val_acc0.4062.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 1.3656 - acc: 0.7086 - val_loss: 2.7969 - val_acc: 0.4062\n",
      "Epoch 28/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.3123 - acc: 0.7223\n",
      "Epoch 00028: val_acc improved from 0.40625 to 0.41563, saving model to benchmark/baseline/freeze-ep-28-val_acc0.4156.hdf5\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 1.3123 - acc: 0.7223 - val_loss: 2.7717 - val_acc: 0.4156\n",
      "Epoch 29/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.2616 - acc: 0.7338\n",
      "Epoch 00029: val_acc improved from 0.41563 to 0.41875, saving model to benchmark/baseline/freeze-ep-29-val_acc0.4187.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 1.2616 - acc: 0.7338 - val_loss: 2.7477 - val_acc: 0.4187\n",
      "Epoch 30/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.2159 - acc: 0.7490\n",
      "Epoch 00030: val_acc improved from 0.41875 to 0.42031, saving model to benchmark/baseline/freeze-ep-30-val_acc0.4203.hdf5\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 1.2159 - acc: 0.7490 - val_loss: 2.7255 - val_acc: 0.4203\n",
      "Epoch 31/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.1695 - acc: 0.7616\n",
      "Epoch 00031: val_acc improved from 0.42031 to 0.42969, saving model to benchmark/baseline/freeze-ep-31-val_acc0.4297.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 1.1695 - acc: 0.7616 - val_loss: 2.7041 - val_acc: 0.4297\n",
      "Epoch 32/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.1209 - acc: 0.7807\n",
      "Epoch 00032: val_acc improved from 0.42969 to 0.43281, saving model to benchmark/baseline/freeze-ep-32-val_acc0.4328.hdf5\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 1.1209 - acc: 0.7807 - val_loss: 2.6841 - val_acc: 0.4328\n",
      "Epoch 33/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.0835 - acc: 0.7910\n",
      "Epoch 00033: val_acc improved from 0.43281 to 0.43594, saving model to benchmark/baseline/freeze-ep-33-val_acc0.4359.hdf5\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 1.0835 - acc: 0.7910 - val_loss: 2.6640 - val_acc: 0.4359\n",
      "Epoch 34/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.0433 - acc: 0.8024\n",
      "Epoch 00034: val_acc improved from 0.43594 to 0.44688, saving model to benchmark/baseline/freeze-ep-34-val_acc0.4469.hdf5\n",
      "82/82 [==============================] - 49s 596ms/step - loss: 1.0433 - acc: 0.8024 - val_loss: 2.6444 - val_acc: 0.4469\n",
      "Epoch 35/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.0075 - acc: 0.8089\n",
      "Epoch 00035: val_acc improved from 0.44688 to 0.45000, saving model to benchmark/baseline/freeze-ep-35-val_acc0.4500.hdf5\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 1.0075 - acc: 0.8089 - val_loss: 2.6276 - val_acc: 0.4500\n",
      "Epoch 36/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.9728 - acc: 0.8227\n",
      "Epoch 00036: val_acc improved from 0.45000 to 0.45156, saving model to benchmark/baseline/freeze-ep-36-val_acc0.4516.hdf5\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 0.9728 - acc: 0.8227 - val_loss: 2.6118 - val_acc: 0.4516\n",
      "Epoch 37/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.9356 - acc: 0.8330\n",
      "Epoch 00037: val_acc improved from 0.45156 to 0.45469, saving model to benchmark/baseline/freeze-ep-37-val_acc0.4547.hdf5\n",
      "82/82 [==============================] - 49s 592ms/step - loss: 0.9356 - acc: 0.8330 - val_loss: 2.5975 - val_acc: 0.4547\n",
      "Epoch 38/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.9043 - acc: 0.8413\n",
      "Epoch 00038: val_acc improved from 0.45469 to 0.45781, saving model to benchmark/baseline/freeze-ep-38-val_acc0.4578.hdf5\n",
      "82/82 [==============================] - 49s 592ms/step - loss: 0.9043 - acc: 0.8413 - val_loss: 2.5839 - val_acc: 0.4578\n",
      "Epoch 39/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.8703 - acc: 0.8528\n",
      "Epoch 00039: val_acc did not improve from 0.45781\n",
      "82/82 [==============================] - 48s 587ms/step - loss: 0.8703 - acc: 0.8528 - val_loss: 2.5725 - val_acc: 0.4578\n",
      "Epoch 40/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.8420 - acc: 0.8593\n",
      "Epoch 00040: val_acc did not improve from 0.45781\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.8420 - acc: 0.8593 - val_loss: 2.5631 - val_acc: 0.4547\n",
      "Epoch 41/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.8141 - acc: 0.8646\n",
      "Epoch 00041: val_acc did not improve from 0.45781\n",
      "82/82 [==============================] - 48s 587ms/step - loss: 0.8141 - acc: 0.8646 - val_loss: 2.5551 - val_acc: 0.4531\n",
      "Epoch 42/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7853 - acc: 0.8722\n",
      "Epoch 00042: val_acc did not improve from 0.45781\n",
      "82/82 [==============================] - 49s 594ms/step - loss: 0.7853 - acc: 0.8722 - val_loss: 2.5464 - val_acc: 0.4547\n",
      "Epoch 43/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7550 - acc: 0.8795\n",
      "Epoch 00043: val_acc improved from 0.45781 to 0.45937, saving model to benchmark/baseline/freeze-ep-43-val_acc0.4594.hdf5\n",
      "82/82 [==============================] - 49s 598ms/step - loss: 0.7550 - acc: 0.8795 - val_loss: 2.5367 - val_acc: 0.4594\n",
      "Epoch 44/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7322 - acc: 0.8856\n",
      "Epoch 00044: val_acc improved from 0.45937 to 0.46719, saving model to benchmark/baseline/freeze-ep-44-val_acc0.4672.hdf5\n",
      "82/82 [==============================] - 49s 599ms/step - loss: 0.7322 - acc: 0.8856 - val_loss: 2.5242 - val_acc: 0.4672\n",
      "Epoch 45/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7091 - acc: 0.8883\n",
      "Epoch 00045: val_acc improved from 0.46719 to 0.47188, saving model to benchmark/baseline/freeze-ep-45-val_acc0.4719.hdf5\n",
      "82/82 [==============================] - 50s 604ms/step - loss: 0.7091 - acc: 0.8883 - val_loss: 2.5103 - val_acc: 0.4719\n",
      "Epoch 46/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6859 - acc: 0.8951\n",
      "Epoch 00046: val_acc improved from 0.47188 to 0.47656, saving model to benchmark/baseline/freeze-ep-46-val_acc0.4766.hdf5\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.6859 - acc: 0.8951 - val_loss: 2.5002 - val_acc: 0.4766\n",
      "Epoch 47/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6618 - acc: 0.9047\n",
      "Epoch 00047: val_acc improved from 0.47656 to 0.47969, saving model to benchmark/baseline/freeze-ep-47-val_acc0.4797.hdf5\n",
      "82/82 [==============================] - 50s 614ms/step - loss: 0.6618 - acc: 0.9047 - val_loss: 2.4896 - val_acc: 0.4797\n",
      "Epoch 48/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6394 - acc: 0.9085\n",
      "Epoch 00048: val_acc improved from 0.47969 to 0.48281, saving model to benchmark/baseline/freeze-ep-48-val_acc0.4828.hdf5\n",
      "82/82 [==============================] - 50s 616ms/step - loss: 0.6394 - acc: 0.9085 - val_loss: 2.4824 - val_acc: 0.4828\n",
      "Epoch 49/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6215 - acc: 0.9119\n",
      "Epoch 00049: val_acc improved from 0.48281 to 0.48438, saving model to benchmark/baseline/freeze-ep-49-val_acc0.4844.hdf5\n",
      "82/82 [==============================] - 49s 602ms/step - loss: 0.6215 - acc: 0.9119 - val_loss: 2.4757 - val_acc: 0.4844\n",
      "Epoch 50/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6021 - acc: 0.9153\n",
      "Epoch 00050: val_acc did not improve from 0.48438\n",
      "82/82 [==============================] - 50s 610ms/step - loss: 0.6021 - acc: 0.9153 - val_loss: 2.4670 - val_acc: 0.4844\n",
      "Epoch 51/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5818 - acc: 0.9211\n",
      "Epoch 00051: val_acc improved from 0.48438 to 0.48594, saving model to benchmark/baseline/freeze-ep-51-val_acc0.4859.hdf5\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.5818 - acc: 0.9211 - val_loss: 2.4599 - val_acc: 0.4859\n",
      "Epoch 52/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5662 - acc: 0.9241\n",
      "Epoch 00052: val_acc improved from 0.48594 to 0.48750, saving model to benchmark/baseline/freeze-ep-52-val_acc0.4875.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 50s 612ms/step - loss: 0.5662 - acc: 0.9241 - val_loss: 2.4523 - val_acc: 0.4875\n",
      "Epoch 53/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5489 - acc: 0.9302\n",
      "Epoch 00053: val_acc improved from 0.48750 to 0.48906, saving model to benchmark/baseline/freeze-ep-53-val_acc0.4891.hdf5\n",
      "82/82 [==============================] - 50s 606ms/step - loss: 0.5489 - acc: 0.9302 - val_loss: 2.4445 - val_acc: 0.4891\n",
      "Epoch 54/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5313 - acc: 0.9317\n",
      "Epoch 00054: val_acc improved from 0.48906 to 0.49219, saving model to benchmark/baseline/freeze-ep-54-val_acc0.4922.hdf5\n",
      "82/82 [==============================] - 50s 612ms/step - loss: 0.5313 - acc: 0.9317 - val_loss: 2.4380 - val_acc: 0.4922\n",
      "Epoch 55/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5148 - acc: 0.9336\n",
      "Epoch 00055: val_acc improved from 0.49219 to 0.49375, saving model to benchmark/baseline/freeze-ep-55-val_acc0.4938.hdf5\n",
      "82/82 [==============================] - 51s 623ms/step - loss: 0.5148 - acc: 0.9336 - val_loss: 2.4329 - val_acc: 0.4938\n",
      "Epoch 56/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4998 - acc: 0.9340\n",
      "Epoch 00056: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 631ms/step - loss: 0.4998 - acc: 0.9340 - val_loss: 2.4284 - val_acc: 0.4922\n",
      "Epoch 57/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4863 - acc: 0.9367\n",
      "Epoch 00057: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 630ms/step - loss: 0.4863 - acc: 0.9367 - val_loss: 2.4261 - val_acc: 0.4922\n",
      "Epoch 58/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4693 - acc: 0.9390\n",
      "Epoch 00058: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 627ms/step - loss: 0.4693 - acc: 0.9390 - val_loss: 2.4261 - val_acc: 0.4891\n",
      "Epoch 59/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4584 - acc: 0.9416\n",
      "Epoch 00059: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 631ms/step - loss: 0.4584 - acc: 0.9416 - val_loss: 2.4313 - val_acc: 0.4906\n",
      "Epoch 60/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4445 - acc: 0.9439\n",
      "Epoch 00060: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 625ms/step - loss: 0.4445 - acc: 0.9439 - val_loss: 2.4358 - val_acc: 0.4859\n",
      "Epoch 61/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4330 - acc: 0.9470\n",
      "Epoch 00061: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 629ms/step - loss: 0.4330 - acc: 0.9470 - val_loss: 2.4425 - val_acc: 0.4781\n",
      "Epoch 62/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4200 - acc: 0.9489\n",
      "Epoch 00062: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 625ms/step - loss: 0.4200 - acc: 0.9489 - val_loss: 2.4497 - val_acc: 0.4766\n",
      "Epoch 63/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4086 - acc: 0.9489\n",
      "Epoch 00063: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 620ms/step - loss: 0.4086 - acc: 0.9489 - val_loss: 2.4632 - val_acc: 0.4781\n",
      "Epoch 64/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3970 - acc: 0.9497\n",
      "Epoch 00064: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 631ms/step - loss: 0.3970 - acc: 0.9497 - val_loss: 2.4773 - val_acc: 0.4750\n",
      "Epoch 65/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3851 - acc: 0.9516\n",
      "Epoch 00065: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 50s 616ms/step - loss: 0.3851 - acc: 0.9516 - val_loss: 2.4885 - val_acc: 0.4719\n",
      "Epoch 66/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3749 - acc: 0.9539\n",
      "Epoch 00066: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 631ms/step - loss: 0.3749 - acc: 0.9539 - val_loss: 2.4980 - val_acc: 0.4688\n",
      "Epoch 67/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3664 - acc: 0.9554\n",
      "Epoch 00067: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 628ms/step - loss: 0.3664 - acc: 0.9554 - val_loss: 2.5029 - val_acc: 0.4703\n",
      "Epoch 68/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3562 - acc: 0.9580\n",
      "Epoch 00068: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 622ms/step - loss: 0.3562 - acc: 0.9580 - val_loss: 2.5020 - val_acc: 0.4719\n",
      "Epoch 69/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3471 - acc: 0.9596\n",
      "Epoch 00069: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 625ms/step - loss: 0.3471 - acc: 0.9596 - val_loss: 2.5043 - val_acc: 0.4703\n",
      "Epoch 70/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3390 - acc: 0.9603\n",
      "Epoch 00070: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 616ms/step - loss: 0.3390 - acc: 0.9603 - val_loss: 2.5027 - val_acc: 0.4734\n",
      "Epoch 71/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3282 - acc: 0.9611\n",
      "Epoch 00071: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 624ms/step - loss: 0.3282 - acc: 0.9611 - val_loss: 2.4943 - val_acc: 0.4781\n",
      "Epoch 72/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3207 - acc: 0.9634\n",
      "Epoch 00072: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 625ms/step - loss: 0.3207 - acc: 0.9634 - val_loss: 2.4800 - val_acc: 0.4734\n",
      "Epoch 73/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3127 - acc: 0.9653\n",
      "Epoch 00073: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 619ms/step - loss: 0.3127 - acc: 0.9653 - val_loss: 2.4709 - val_acc: 0.4766\n",
      "Epoch 74/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3053 - acc: 0.9645\n",
      "Epoch 00074: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 52s 632ms/step - loss: 0.3053 - acc: 0.9645 - val_loss: 2.4573 - val_acc: 0.4781\n",
      "Epoch 75/75\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2978 - acc: 0.9653\n",
      "Epoch 00075: val_acc did not improve from 0.49375\n",
      "82/82 [==============================] - 51s 620ms/step - loss: 0.2978 - acc: 0.9653 - val_loss: 2.4429 - val_acc: 0.4812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb328302b00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path('benchmark/baseline-augment')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model.fit(train_gen,\n",
    "          steps_per_epoch=train_datagen.number_step,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_datagen.number_step,\n",
    "          epochs=75,\n",
    "          callbacks=[\n",
    "              TensorBoard(str(model_dir)),\n",
    "              ModelCheckpoint(str(model_dir / 'freeze-ep-{epoch:02d}-val_acc{val_acc:.4f}.hdf5'),\n",
    "                        monitor='val_acc',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='max')\n",
    "          ]\n",
    "          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found it start to overfiting at early epochs and early train acc.\n",
    "May be this model didn't learn true way that we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path('datasets/train/train')\n",
    "\n",
    "train_df = pd.read_csv('datasets/train_sets.csv')\n",
    "train_datagen = DataGenerator(train_df, input_dir)\n",
    "train_gen = train_datagen.create_generator(shuffle=False)\n",
    "\n",
    "val_df = pd.read_csv('datasets/val_sets.csv')\n",
    "val_datagen = DataGenerator(val_df, input_dir)\n",
    "val_gen = val_datagen.create_generator(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 15, 30, 1024)      7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 313)               320825    \n",
      "=================================================================\n",
      "Total params: 7,358,329\n",
      "Trainable params: 320,825\n",
      "Non-trainable params: 7,037,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('benchmark/baseline/freeze-ep-55-val_acc0.4938.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(datagen, gen):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for i in tqdm_notebook(range(datagen.number_step)):\n",
    "        xs, _labels = next(gen)\n",
    "        _pred = model.predict(xs, verbose=0)\n",
    "\n",
    "        labels.extend(_labels)\n",
    "        predictions.extend(_pred)\n",
    "    labels = np.array(labels).argmax(axis=1)\n",
    "    predictions = np.array(predictions).argmax(axis=1)\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadf6b8196be47b9b970207ee1b9cfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=82.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c8decb97e54fd093f9802003ca6d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_predictions = evaluate(train_datagen, train_gen)\n",
    "val_labels, val_predictions = evaluate(val_datagen, val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "class 0 is new_whale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       642\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       1.00      0.75      0.86         4\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      0.75      0.86         4\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       1.00      1.00      1.00         6\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         6\n",
      "          13       1.00      1.00      1.00        13\n",
      "          14       1.00      1.00      1.00         4\n",
      "          15       1.00      1.00      1.00         6\n",
      "          16       1.00      0.75      0.86         4\n",
      "          17       1.00      0.93      0.96        27\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         4\n",
      "          21       1.00      1.00      1.00         5\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       1.00      0.69      0.82        13\n",
      "          24       1.00      1.00      1.00         4\n",
      "          25       1.00      1.00      1.00         4\n",
      "          26       0.85      0.85      0.85        13\n",
      "          27       1.00      1.00      1.00         4\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       1.00      1.00      1.00         7\n",
      "          30       1.00      0.62      0.77         8\n",
      "          31       1.00      1.00      1.00         5\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      0.75      0.86         4\n",
      "          35       0.85      0.94      0.89        18\n",
      "          36       1.00      1.00      1.00         9\n",
      "          37       1.00      1.00      1.00         5\n",
      "          38       1.00      0.83      0.91         6\n",
      "          39       1.00      1.00      1.00         4\n",
      "          40       1.00      1.00      1.00         4\n",
      "          41       1.00      1.00      1.00         4\n",
      "          42       1.00      0.75      0.86         4\n",
      "          43       1.00      1.00      1.00         3\n",
      "          44       1.00      1.00      1.00         4\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      1.00      1.00         4\n",
      "          47       1.00      0.91      0.95        11\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       1.00      1.00      1.00         4\n",
      "          50       1.00      1.00      1.00         5\n",
      "          51       1.00      1.00      1.00         4\n",
      "          52       1.00      1.00      1.00         6\n",
      "          53       0.75      1.00      0.86         6\n",
      "          54       1.00      1.00      1.00         3\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       0.83      1.00      0.91         5\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      1.00      1.00         5\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      1.00      1.00         8\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       1.00      0.83      0.91         6\n",
      "          63       1.00      1.00      1.00         8\n",
      "          64       1.00      1.00      1.00         4\n",
      "          65       1.00      1.00      1.00         4\n",
      "          66       1.00      1.00      1.00         7\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       1.00      0.50      0.67         8\n",
      "          69       1.00      1.00      1.00         9\n",
      "          70       1.00      1.00      1.00         4\n",
      "          71       1.00      1.00      1.00         4\n",
      "          72       1.00      1.00      1.00         6\n",
      "          73       1.00      0.67      0.80         6\n",
      "          74       1.00      1.00      1.00         5\n",
      "          75       1.00      1.00      1.00         6\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       0.93      0.88      0.90        16\n",
      "          78       1.00      1.00      1.00         6\n",
      "          79       0.83      1.00      0.91        10\n",
      "          80       1.00      0.83      0.91         6\n",
      "          81       1.00      1.00      1.00         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      0.80      0.89         5\n",
      "          84       1.00      1.00      1.00         9\n",
      "          85       1.00      1.00      1.00         5\n",
      "          86       1.00      1.00      1.00        10\n",
      "          87       1.00      1.00      1.00         6\n",
      "          88       0.91      1.00      0.95        10\n",
      "          89       1.00      1.00      1.00         6\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       1.00      1.00      1.00         7\n",
      "          92       0.88      0.88      0.88         8\n",
      "          93       0.83      1.00      0.91         5\n",
      "          94       1.00      1.00      1.00         4\n",
      "          95       1.00      1.00      1.00         5\n",
      "          96       1.00      1.00      1.00         3\n",
      "          97       0.80      1.00      0.89         4\n",
      "          98       1.00      1.00      1.00         4\n",
      "          99       1.00      1.00      1.00        11\n",
      "         100       1.00      1.00      1.00         6\n",
      "         101       1.00      1.00      1.00         4\n",
      "         102       1.00      1.00      1.00         4\n",
      "         103       1.00      1.00      1.00         6\n",
      "         104       1.00      1.00      1.00         5\n",
      "         105       1.00      1.00      1.00         6\n",
      "         106       0.86      1.00      0.92         6\n",
      "         107       1.00      1.00      1.00         5\n",
      "         108       1.00      1.00      1.00         6\n",
      "         109       1.00      1.00      1.00         6\n",
      "         110       1.00      1.00      1.00         4\n",
      "         111       1.00      0.90      0.95        10\n",
      "         112       1.00      1.00      1.00         4\n",
      "         113       0.86      1.00      0.92         6\n",
      "         114       1.00      1.00      1.00         5\n",
      "         115       1.00      1.00      1.00         4\n",
      "         116       1.00      1.00      1.00         6\n",
      "         117       1.00      1.00      1.00         4\n",
      "         118       1.00      1.00      1.00         7\n",
      "         119       1.00      1.00      1.00         4\n",
      "         120       1.00      1.00      1.00         8\n",
      "         121       1.00      0.67      0.80         6\n",
      "         122       1.00      1.00      1.00         5\n",
      "         123       1.00      1.00      1.00        13\n",
      "         124       1.00      1.00      1.00         6\n",
      "         125       1.00      1.00      1.00        16\n",
      "         126       1.00      1.00      1.00         8\n",
      "         127       1.00      0.85      0.92        13\n",
      "         128       1.00      1.00      1.00         4\n",
      "         129       0.80      1.00      0.89         4\n",
      "         130       1.00      1.00      1.00         5\n",
      "         131       0.80      0.80      0.80         5\n",
      "         132       1.00      1.00      1.00         5\n",
      "         133       1.00      1.00      1.00         5\n",
      "         134       1.00      0.50      0.67         4\n",
      "         135       1.00      1.00      1.00         4\n",
      "         136       1.00      1.00      1.00         9\n",
      "         137       1.00      1.00      1.00         4\n",
      "         138       1.00      0.94      0.97        17\n",
      "         139       1.00      0.75      0.86         4\n",
      "         140       0.95      1.00      0.98        20\n",
      "         141       1.00      1.00      1.00         4\n",
      "         142       1.00      1.00      1.00         6\n",
      "         143       0.83      1.00      0.91         5\n",
      "         144       1.00      1.00      1.00         5\n",
      "         145       1.00      1.00      1.00         4\n",
      "         146       1.00      1.00      1.00         5\n",
      "         147       1.00      1.00      1.00         9\n",
      "         148       1.00      0.62      0.77         8\n",
      "         149       1.00      1.00      1.00         5\n",
      "         150       1.00      0.91      0.95        11\n",
      "         151       1.00      1.00      1.00         5\n",
      "         152       0.88      1.00      0.93         7\n",
      "         153       1.00      1.00      1.00         4\n",
      "         154       1.00      1.00      1.00         4\n",
      "         155       1.00      1.00      1.00         4\n",
      "         156       1.00      0.86      0.92         7\n",
      "         157       1.00      0.83      0.91         6\n",
      "         158       1.00      1.00      1.00         5\n",
      "         159       1.00      1.00      1.00         5\n",
      "         160       1.00      0.71      0.83         7\n",
      "         161       1.00      0.75      0.86         4\n",
      "         162       0.92      1.00      0.96        12\n",
      "         163       1.00      1.00      1.00         5\n",
      "         164       1.00      1.00      1.00         6\n",
      "         165       1.00      0.44      0.62         9\n",
      "         166       1.00      1.00      1.00        11\n",
      "         167       1.00      1.00      1.00         5\n",
      "         168       1.00      0.86      0.92         7\n",
      "         169       1.00      1.00      1.00         4\n",
      "         170       1.00      1.00      1.00         4\n",
      "         171       1.00      1.00      1.00         6\n",
      "         172       1.00      1.00      1.00         4\n",
      "         173       1.00      1.00      1.00         4\n",
      "         174       1.00      1.00      1.00         4\n",
      "         175       1.00      1.00      1.00         4\n",
      "         176       1.00      1.00      1.00         4\n",
      "         177       0.94      1.00      0.97        15\n",
      "         178       1.00      1.00      1.00         6\n",
      "         179       1.00      0.86      0.92         7\n",
      "         180       1.00      1.00      1.00         6\n",
      "         181       1.00      0.59      0.74        17\n",
      "         182       1.00      1.00      1.00        20\n",
      "         183       1.00      0.80      0.89         5\n",
      "         184       1.00      1.00      1.00         4\n",
      "         185       0.88      0.93      0.90        15\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      1.00      1.00         4\n",
      "         188       1.00      1.00      1.00        12\n",
      "         189       1.00      0.80      0.89         5\n",
      "         190       1.00      0.83      0.91         6\n",
      "         191       1.00      1.00      1.00         4\n",
      "         192       1.00      1.00      1.00         4\n",
      "         193       1.00      1.00      1.00         9\n",
      "         194       1.00      1.00      1.00         5\n",
      "         195       1.00      1.00      1.00         6\n",
      "         196       1.00      1.00      1.00         6\n",
      "         197       1.00      1.00      1.00         4\n",
      "         198       1.00      1.00      1.00         9\n",
      "         199       1.00      1.00      1.00        12\n",
      "         200       1.00      0.67      0.80         6\n",
      "         201       1.00      1.00      1.00         4\n",
      "         202       1.00      1.00      1.00         6\n",
      "         203       1.00      1.00      1.00         4\n",
      "         204       1.00      1.00      1.00         4\n",
      "         205       1.00      0.82      0.90        17\n",
      "         206       1.00      1.00      1.00         4\n",
      "         207       1.00      1.00      1.00         7\n",
      "         208       1.00      1.00      1.00         7\n",
      "         209       1.00      1.00      1.00         5\n",
      "         210       1.00      1.00      1.00         4\n",
      "         211       1.00      1.00      1.00         6\n",
      "         212       1.00      1.00      1.00        11\n",
      "         213       1.00      1.00      1.00        13\n",
      "         214       0.83      1.00      0.91         5\n",
      "         215       1.00      1.00      1.00         4\n",
      "         216       0.88      1.00      0.93         7\n",
      "         217       0.83      1.00      0.91         5\n",
      "         218       1.00      1.00      1.00         6\n",
      "         219       1.00      0.89      0.94         9\n",
      "         220       1.00      1.00      1.00         5\n",
      "         221       1.00      1.00      1.00         6\n",
      "         222       1.00      1.00      1.00         4\n",
      "         223       1.00      0.86      0.92        14\n",
      "         224       1.00      1.00      1.00         5\n",
      "         225       1.00      0.89      0.94         9\n",
      "         226       1.00      1.00      1.00         6\n",
      "         227       1.00      0.78      0.88         9\n",
      "         228       1.00      1.00      1.00         4\n",
      "         229       1.00      1.00      1.00         5\n",
      "         230       0.91      1.00      0.95        10\n",
      "         231       1.00      1.00      1.00         4\n",
      "         232       1.00      1.00      1.00         4\n",
      "         233       1.00      1.00      1.00         4\n",
      "         234       1.00      0.80      0.89         5\n",
      "         235       1.00      1.00      1.00         4\n",
      "         236       1.00      0.86      0.92        14\n",
      "         237       1.00      1.00      1.00         4\n",
      "         238       1.00      1.00      1.00         4\n",
      "         239       1.00      1.00      1.00         7\n",
      "         240       0.83      1.00      0.91         5\n",
      "         241       1.00      1.00      1.00         6\n",
      "         242       0.80      1.00      0.89         4\n",
      "         243       0.90      1.00      0.95         9\n",
      "         244       0.92      1.00      0.96        11\n",
      "         245       1.00      1.00      1.00         4\n",
      "         246       1.00      1.00      1.00         4\n",
      "         247       1.00      1.00      1.00         3\n",
      "         248       1.00      1.00      1.00         4\n",
      "         249       1.00      1.00      1.00         4\n",
      "         250       1.00      0.75      0.86         4\n",
      "         251       1.00      1.00      1.00         4\n",
      "         252       1.00      1.00      1.00         5\n",
      "         253       1.00      1.00      1.00         5\n",
      "         254       0.80      1.00      0.89         4\n",
      "         255       1.00      1.00      1.00         4\n",
      "         256       1.00      0.33      0.50         6\n",
      "         257       1.00      1.00      1.00         6\n",
      "         258       1.00      1.00      1.00         4\n",
      "         259       1.00      1.00      1.00         4\n",
      "         260       1.00      1.00      1.00         5\n",
      "         261       1.00      1.00      1.00         7\n",
      "         262       1.00      1.00      1.00         4\n",
      "         263       1.00      1.00      1.00         4\n",
      "         264       1.00      1.00      1.00         7\n",
      "         265       0.86      1.00      0.92         6\n",
      "         266       0.86      1.00      0.92         6\n",
      "         267       1.00      1.00      1.00         7\n",
      "         268       1.00      1.00      1.00         6\n",
      "         269       0.90      1.00      0.95         9\n",
      "         270       1.00      1.00      1.00         5\n",
      "         271       1.00      1.00      1.00         5\n",
      "         272       1.00      1.00      1.00        13\n",
      "         273       1.00      1.00      1.00         3\n",
      "         274       1.00      1.00      1.00         6\n",
      "         275       1.00      0.80      0.89         5\n",
      "         276       1.00      0.83      0.91         6\n",
      "         277       1.00      0.86      0.92         7\n",
      "         278       1.00      1.00      1.00         4\n",
      "         279       1.00      0.83      0.91         6\n",
      "         280       1.00      1.00      1.00         4\n",
      "         281       0.86      1.00      0.92         6\n",
      "         282       0.83      1.00      0.91         5\n",
      "         283       1.00      1.00      1.00         4\n",
      "         284       1.00      1.00      1.00         4\n",
      "         285       0.90      1.00      0.95         9\n",
      "         286       0.80      1.00      0.89         4\n",
      "         287       0.83      1.00      0.91         5\n",
      "         288       1.00      1.00      1.00         4\n",
      "         289       1.00      1.00      1.00        13\n",
      "         290       0.92      1.00      0.96        11\n",
      "         291       1.00      1.00      1.00         4\n",
      "         292       1.00      0.75      0.86         4\n",
      "         293       1.00      1.00      1.00         4\n",
      "         294       0.80      1.00      0.89         4\n",
      "         295       1.00      0.83      0.91         6\n",
      "         296       1.00      1.00      1.00        16\n",
      "         297       1.00      1.00      1.00         5\n",
      "         298       1.00      1.00      1.00         5\n",
      "         299       1.00      1.00      1.00         5\n",
      "         300       1.00      0.83      0.91         6\n",
      "         301       1.00      1.00      1.00         5\n",
      "         302       0.91      1.00      0.95        10\n",
      "         303       1.00      1.00      1.00         7\n",
      "         304       1.00      0.90      0.95        10\n",
      "         305       1.00      1.00      1.00         6\n",
      "         306       1.00      1.00      1.00         5\n",
      "         307       0.89      0.94      0.91        17\n",
      "         308       1.00      1.00      1.00        10\n",
      "         309       1.00      1.00      1.00         7\n",
      "         310       1.00      1.00      1.00         4\n",
      "         311       1.00      1.00      1.00         4\n",
      "         312       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.95      2624\n",
      "   macro avg       0.98      0.96      0.97      2624\n",
      "weighted avg       0.95      0.95      0.95      2624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train set')\n",
    "print(classification_report(train_labels, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.86      0.51       158\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.60      0.75      0.67         4\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.67      0.67      0.67         6\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       0.00      0.00      0.00         2\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       1.00      0.33      0.50         3\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.50      0.25      0.33         4\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       1.00      1.00      1.00         2\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       0.00      0.00      0.00         1\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      0.50      0.67         2\n",
      "          34       1.00      1.00      1.00         1\n",
      "          35       0.38      0.60      0.46         5\n",
      "          36       0.75      1.00      0.86         3\n",
      "          37       1.00      1.00      1.00         1\n",
      "          38       1.00      0.50      0.67         2\n",
      "          39       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       1.00      1.00      1.00         1\n",
      "          43       1.00      1.00      1.00         1\n",
      "          44       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          47       0.67      0.67      0.67         3\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         1\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       1.00      1.00      1.00         1\n",
      "          53       1.00      0.50      0.67         2\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         1\n",
      "          56       1.00      1.00      1.00         1\n",
      "          57       1.00      1.00      1.00         1\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       1.00      1.00      1.00         1\n",
      "          60       1.00      0.50      0.67         2\n",
      "          61       0.00      0.00      0.00         1\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       1.00      0.50      0.67         2\n",
      "          64       0.00      0.00      0.00         1\n",
      "          65       0.00      0.00      0.00         1\n",
      "          66       0.50      0.50      0.50         2\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       1.00      0.50      0.67         2\n",
      "          69       1.00      0.50      0.67         2\n",
      "          70       1.00      1.00      1.00         1\n",
      "          71       1.00      1.00      1.00         1\n",
      "          72       1.00      0.50      0.67         2\n",
      "          73       1.00      1.00      1.00         1\n",
      "          74       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          76       1.00      1.00      1.00         1\n",
      "          77       0.25      0.33      0.29         3\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       1.00      1.00      1.00         3\n",
      "          80       1.00      0.50      0.67         2\n",
      "          81       0.00      0.00      0.00         1\n",
      "          82       0.00      0.00      0.00         1\n",
      "          83       1.00      1.00      1.00         1\n",
      "          84       1.00      0.67      0.80         3\n",
      "          85       1.00      1.00      1.00         1\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.00      0.00      0.00         1\n",
      "          88       1.00      0.50      0.67         2\n",
      "          89       0.00      0.00      0.00         1\n",
      "          90       0.00      0.00      0.00         1\n",
      "          91       0.00      0.00      0.00         2\n",
      "          92       1.00      0.50      0.67         2\n",
      "          93       0.00      0.00      0.00         1\n",
      "          94       0.00      0.00      0.00         1\n",
      "          95       1.00      0.50      0.67         2\n",
      "          96       0.00      0.00      0.00         1\n",
      "          97       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00         1\n",
      "          99       0.75      1.00      0.86         3\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         1\n",
      "         102       0.00      0.00      0.00         1\n",
      "         103       0.00      0.00      0.00         2\n",
      "         104       1.00      1.00      1.00         1\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.00      0.00      0.00         1\n",
      "         107       1.00      1.00      1.00         1\n",
      "         108       0.00      0.00      0.00         1\n",
      "         109       1.00      1.00      1.00         1\n",
      "         110       1.00      1.00      1.00         1\n",
      "         111       0.00      0.00      0.00         3\n",
      "         112       0.00      0.00      0.00         0\n",
      "         113       0.00      0.00      0.00         1\n",
      "         114       0.00      0.00      0.00         1\n",
      "         115       1.00      1.00      1.00         1\n",
      "         116       0.00      0.00      0.00         1\n",
      "         117       1.00      1.00      1.00         1\n",
      "         118       0.00      0.00      0.00         2\n",
      "         119       1.00      1.00      1.00         1\n",
      "         120       1.00      0.50      0.67         2\n",
      "         121       1.00      0.50      0.67         2\n",
      "         122       1.00      1.00      1.00         1\n",
      "         123       1.00      0.67      0.80         3\n",
      "         124       0.00      0.00      0.00         1\n",
      "         125       0.50      0.40      0.44         5\n",
      "         126       0.00      0.00      0.00         2\n",
      "         127       0.00      0.00      0.00         1\n",
      "         128       1.00      1.00      1.00         1\n",
      "         129       1.00      1.00      1.00         1\n",
      "         130       0.00      0.00      0.00         1\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.00      0.00      0.00         1\n",
      "         133       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00         1\n",
      "         136       0.50      0.50      0.50         2\n",
      "         138       1.00      0.50      0.67         4\n",
      "         139       0.00      0.00      0.00         1\n",
      "         140       0.25      0.75      0.38         4\n",
      "         141       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         2\n",
      "         143       0.00      0.00      0.00         2\n",
      "         144       1.00      1.00      1.00         1\n",
      "         145       1.00      1.00      1.00         1\n",
      "         146       0.00      0.00      0.00         1\n",
      "         147       1.00      0.67      0.80         3\n",
      "         148       0.00      0.00      0.00         3\n",
      "         149       0.00      0.00      0.00         1\n",
      "         150       0.67      0.67      0.67         3\n",
      "         151       1.00      1.00      1.00         1\n",
      "         152       0.00      0.00      0.00         2\n",
      "         153       1.00      1.00      1.00         1\n",
      "         154       1.00      1.00      1.00         1\n",
      "         155       0.00      0.00      0.00         1\n",
      "         156       1.00      0.50      0.67         2\n",
      "         157       0.00      0.00      0.00         1\n",
      "         158       0.00      0.00      0.00         1\n",
      "         159       1.00      1.00      1.00         1\n",
      "         160       1.00      0.50      0.67         2\n",
      "         161       1.00      1.00      1.00         1\n",
      "         162       0.00      0.00      0.00         3\n",
      "         163       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         2\n",
      "         166       1.00      0.67      0.80         3\n",
      "         167       0.00      0.00      0.00         1\n",
      "         168       1.00      0.50      0.67         2\n",
      "         169       0.00      0.00      0.00         1\n",
      "         170       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         1\n",
      "         172       1.00      1.00      1.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       1.00      1.00      1.00         1\n",
      "         175       1.00      1.00      1.00         1\n",
      "         176       0.00      0.00      0.00         1\n",
      "         177       0.33      0.25      0.29         4\n",
      "         178       1.00      1.00      1.00         1\n",
      "         179       0.00      0.00      0.00         1\n",
      "         180       1.00      0.50      0.67         2\n",
      "         181       0.00      0.00      0.00         4\n",
      "         182       0.57      0.67      0.62         6\n",
      "         183       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         1\n",
      "         185       1.00      0.25      0.40         4\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         1\n",
      "         188       1.00      0.67      0.80         3\n",
      "         190       1.00      0.50      0.67         2\n",
      "         191       0.00      0.00      0.00         1\n",
      "         192       0.00      0.00      0.00         1\n",
      "         193       0.25      0.50      0.33         2\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.00      0.00      0.00         1\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.67      1.00      0.80         2\n",
      "         199       0.67      0.67      0.67         3\n",
      "         200       1.00      1.00      1.00         1\n",
      "         201       1.00      1.00      1.00         1\n",
      "         202       0.00      0.00      0.00         2\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       1.00      1.00      1.00         1\n",
      "         205       0.80      0.80      0.80         5\n",
      "         206       1.00      1.00      1.00         1\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       1.00      0.50      0.67         2\n",
      "         209       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         1\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.33      0.50         3\n",
      "         213       1.00      0.33      0.50         3\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       1.00      1.00      1.00         1\n",
      "         216       1.00      0.50      0.67         2\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       1.00      1.00      1.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       1.00      1.00      1.00         1\n",
      "         221       0.00      0.00      0.00         2\n",
      "         222       1.00      1.00      1.00         1\n",
      "         223       0.00      0.00      0.00         4\n",
      "         224       0.00      0.00      0.00         1\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       0.00      0.00      0.00         2\n",
      "         227       0.00      0.00      0.00         2\n",
      "         228       0.00      0.00      0.00         1\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.50      0.33      0.40         3\n",
      "         231       0.00      0.00      0.00         1\n",
      "         232       0.00      0.00      0.00         1\n",
      "         233       0.00      0.00      0.00         1\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       1.00      1.00      1.00         1\n",
      "         236       0.60      0.75      0.67         4\n",
      "         237       1.00      1.00      1.00         1\n",
      "         238       0.00      0.00      0.00         1\n",
      "         239       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       1.00      1.00      1.00         1\n",
      "         243       0.00      0.00      0.00         3\n",
      "         244       0.67      0.67      0.67         3\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       1.00      1.00      1.00         1\n",
      "         247       0.00      0.00      0.00         1\n",
      "         248       1.00      1.00      1.00         1\n",
      "         249       1.00      1.00      1.00         1\n",
      "         250       1.00      1.00      1.00         1\n",
      "         251       0.00      0.00      0.00         1\n",
      "         252       0.00      0.00      0.00         1\n",
      "         253       0.00      0.00      0.00         1\n",
      "         254       1.00      1.00      1.00         1\n",
      "         255       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         2\n",
      "         257       1.00      1.00      1.00         1\n",
      "         258       0.50      1.00      0.67         1\n",
      "         259       0.00      0.00      0.00         1\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       1.00      0.50      0.67         2\n",
      "         262       0.00      0.00      0.00         1\n",
      "         263       1.00      1.00      1.00         1\n",
      "         264       1.00      0.50      0.67         2\n",
      "         265       0.00      0.00      0.00         1\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       1.00      0.50      0.67         2\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       1.00      1.00      1.00         2\n",
      "         270       1.00      1.00      1.00         1\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       0.33      0.33      0.33         3\n",
      "         273       1.00      1.00      1.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.00      0.00      0.00         1\n",
      "         276       0.00      0.00      0.00         2\n",
      "         277       0.00      0.00      0.00         2\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       1.00      1.00      1.00         1\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       1.00      1.00      1.00         1\n",
      "         284       0.00      0.00      0.00         1\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       1.00      1.00      1.00         1\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       1.00      0.50      0.67         4\n",
      "         290       0.00      0.00      0.00         3\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       1.00      1.00      1.00         1\n",
      "         296       1.00      0.75      0.86         4\n",
      "         297       1.00      1.00      1.00         1\n",
      "         298       1.00      1.00      1.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         2\n",
      "         301       1.00      1.00      1.00         1\n",
      "         302       0.00      0.00      0.00         3\n",
      "         303       1.00      0.50      0.67         2\n",
      "         304       0.00      0.00      0.00         3\n",
      "         305       0.00      0.00      0.00         1\n",
      "         306       1.00      1.00      1.00         1\n",
      "         307       0.67      0.80      0.73         5\n",
      "         308       1.00      0.67      0.80         3\n",
      "         309       0.00      0.00      0.00         2\n",
      "         310       0.00      0.00      0.00         1\n",
      "         311       0.00      0.00      0.00         1\n",
      "         312       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.49       640\n",
      "   macro avg       0.41      0.35      0.37       640\n",
      "weighted avg       0.43      0.49      0.42       640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Val set')\n",
    "print(classification_report(val_labels, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Validation set.\n",
    "Recall of new whale (class 0) is too much greater than precision. Maybe this model is bias to predict new whale (class 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With a very unbalance dataset, the CNN+FC model seems didn't work well. Whale images of any ID are very similar.\n",
    "May this model is more suitable with an image classification dataset that obviously class of images.\n",
    "\n",
    "Some one-shot learning model like Siamese Network may have greater performance than this model because its aim to classify the different feature between 2 images.\n",
    "\n",
    "In the real-world, this model doesn't suitable because not easy to adapt to a new registered whale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
